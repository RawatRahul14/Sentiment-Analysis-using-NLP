{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3 style=\"color:red;\">Sentiment analysis system</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives:**\n",
    "\n",
    "- The goal of this assignment is to build a sentiment analysis system using the Bag of Words (BoW) and TF-IDF techniques. Students will preprocess the dataset, clean and tokenize text using regular expressions (regex) in Python, and apply at least three machine learning models to classify the sentiment of given text data. Finally, they will evaluate and compare model performances to determine the best-performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Data Preprocessing & Cleaning (30 Marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Horrible!!! The worst experience ever. Do not ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Terrible service!! I won't buy from here again...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I had high hopes, but it broke after a week. :-/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product is okay, but packaging was awful. ?!?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0                                                NaN          0\n",
       "1  Horrible!!! The worst experience ever. Do not ...          0\n",
       "2  Terrible service!! I won't buy from here again...          0\n",
       "3   I had high hopes, but it broke after a week. :-/          0\n",
       "4      Product is okay, but packaging was awful. ?!?          0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../Data/Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       980 non-null    object\n",
      " 1   sentiment  1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justification**: Missing values can lead to errors during analysis. Identifying them early ensures proper handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text         20\n",
      "sentiment     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Horrible!!! The worst experience ever. Do not ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Terrible service!! I won't buy from here again...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I had high hopes, but it broke after a week. :-/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Product is okay, but packaging was awful. ?!?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good quality, but a bit expensive. Worth it th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  Horrible!!! The worst experience ever. Do not ...          0\n",
       "1  Terrible service!! I won't buy from here again...          0\n",
       "2   I had high hopes, but it broke after a week. :-/          0\n",
       "3      Product is okay, but packaging was awful. ?!?          0\n",
       "4  Good quality, but a bit expensive. Worth it th...          0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with missing values in the 'text' or 'sentiment' columns\n",
    "df.dropna(subset=['text', 'sentiment'], inplace=True)\n",
    "\n",
    "# Reset the index after dropping rows\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justification:** Missing sentiment labels cannot be inferred, and missing text data cannot be used for analysis. Dropping these rows ensures data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique text entries: 20\n",
      "Number of duplicate text entries: 960\n"
     ]
    }
   ],
   "source": [
    "#Count the number of unique text in the dataset\n",
    "unique_text_count = df['text'].nunique()\n",
    "print(f\"Number of unique text entries: {unique_text_count}\")\n",
    "\n",
    "# Count the number of duplicates text in the dataset\n",
    "duplicates_count = df['text'].duplicated().sum()\n",
    "print(f\"Number of duplicate text entries: {duplicates_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Drop duplicates\n",
    "# df.drop_duplicates(subset=['text'], inplace=True)\n",
    "# # Reset the index after dropping duplicates\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justification**: Dropping duplicate text entries in sentiment analysis ensures that the dataset is clean and free from redundant data and avoids bias and reduces overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Horrible The worst experience ever Do not buy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Terrible service I wont buy from here again</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I had high hopes but it broke after a week</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Product is okay but packaging was awful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good quality but a bit expensive Worth it though</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text  sentiment\n",
       "0     Horrible The worst experience ever Do not buy          0\n",
       "1       Terrible service I wont buy from here again          0\n",
       "2        I had high hopes but it broke after a week          0\n",
       "3           Product is okay but packaging was awful          0\n",
       "4  Good quality but a bit expensive Worth it though          0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove non-alphabetic characters and ASCII codes\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'text' column\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justification:** Non-alphabetic characters and ASCII codes do not contribute to sentiment analysis and can introduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>horrible the worst experience ever do not buy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>terrible service i wont buy from here again</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i had high hopes but it broke after a week</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>product is okay but packaging was awful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good quality but a bit expensive worth it though</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text  sentiment\n",
       "0     horrible the worst experience ever do not buy          0\n",
       "1       terrible service i wont buy from here again          0\n",
       "2        i had high hopes but it broke after a week          0\n",
       "3           product is okay but packaging was awful          0\n",
       "4  good quality but a bit expensive worth it though          0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justification:** Lowercasing ensures that words like \"Happy\" and \"happy\" are treated as the same token.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rajprasadshrestha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>horrible worst experience ever buy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>terrible service wont buy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high hopes broke week</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>product okay packaging awful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good quality bit expensive worth though</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text  sentiment\n",
       "0       horrible worst experience ever buy          0\n",
       "1                terrible service wont buy          0\n",
       "2                    high hopes broke week          0\n",
       "3             product okay packaging awful          0\n",
       "4  good quality bit expensive worth though          0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "# Apply the stopwords removal function\n",
    "df['text'] = df['text'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justification:** Stopwords do not carry significant meaning and can be removed to reduce dimensionality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rajprasadshrestha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>horrible worst experience ever buy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>terrible service wont buy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high hope broke week</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>product okay packaging awful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good quality bit expensive worth though</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text  sentiment\n",
       "0       horrible worst experience ever buy          0\n",
       "1                terrible service wont buy          0\n",
       "2                     high hope broke week          0\n",
       "3             product okay packaging awful          0\n",
       "4  good quality bit expensive worth though          0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet if not already downloaded\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "# Apply the lemmatization function\n",
    "df['text'] = df['text'].apply(lemmatize_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justification:** Lemmatization reduces words to their base forms, ensuring that different forms of the same word are treated as one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of Preprocessing Steps**\n",
    "\n",
    "- Handled Missing Values: Dropped rows with missing values in the text or sentiment columns to ensure data quality.\n",
    "\n",
    "- Removed Non-Alphabetic Characters: Used regex to remove special characters, ASCII codes, and extra spaces.\n",
    "\n",
    "- Converted Text to Lowercase: Ensured uniformity in the text data.\n",
    "\n",
    "- Removed Stopwords: Eliminated common words that do not contribute to sentiment analysis.\n",
    "\n",
    "- Performed Lemmatization: Normalized words to their base forms for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Feature Engineering using NLP Techniques (20 Marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement Bag of Words (BoW) for text representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the bag of words matrix: (980, 76)\n"
     ]
    }
   ],
   "source": [
    "#Implement bag of words in df[text]\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "bagofword_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "X_bagofword = bagofword_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "print(f\"Shape of the bag of words matrix: {X_bagofword.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement TF-IDF for text representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TF-IDF matrix: (980, 76)\n"
     ]
    }
   ],
   "source": [
    "#implement TF-IDF in df[text]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['text'])\n",
    "print(f\"Shape of the TF-IDF matrix: {X_tfidf.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparision of TF-IDF and Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     absolutely  advertised  amazing  arrived  away  awful  best  better  bit  \\\n",
      "0             0           0        0        0     0      0     0       0    0   \n",
      "1             0           0        0        0     0      0     0       0    0   \n",
      "2             0           0        0        0     0      0     0       0    0   \n",
      "3             0           0        0        0     0      1     0       0    0   \n",
      "4             0           0        0        0     0      0     0       0    1   \n",
      "..          ...         ...      ...      ...   ...    ...   ...     ...  ...   \n",
      "975           0           0        0        0     0      0     0       0    0   \n",
      "976           0           0        0        0     0      0     0       0    0   \n",
      "977           0           0        0        0     0      0     0       0    0   \n",
      "978           1           0        0        0     0      0     0       0    0   \n",
      "979           0           0        0        0     0      0     0       0    0   \n",
      "\n",
      "     broke  ...  time  took  trust  week  wont  work  worst  worth  worthit  \\\n",
      "0        0  ...     0     0      0     0     0     0      1      0        0   \n",
      "1        0  ...     0     0      0     0     1     0      0      0        0   \n",
      "2        1  ...     0     0      0     1     0     0      0      0        0   \n",
      "3        0  ...     0     0      0     0     0     0      0      0        0   \n",
      "4        0  ...     0     0      0     0     0     0      0      1        0   \n",
      "..     ...  ...   ...   ...    ...   ...   ...   ...    ...    ...      ...   \n",
      "975      0  ...     0     0      0     0     0     1      0      0        0   \n",
      "976      0  ...     0     0      0     0     0     0      0      0        0   \n",
      "977      0  ...     0     0      0     0     0     0      0      0        0   \n",
      "978      0  ...     0     0      0     0     0     0      0      0        0   \n",
      "979      0  ...     0     0      0     0     0     0      0      0        0   \n",
      "\n",
      "     would  \n",
      "0        0  \n",
      "1        0  \n",
      "2        0  \n",
      "3        0  \n",
      "4        0  \n",
      "..     ...  \n",
      "975      0  \n",
      "976      0  \n",
      "977      0  \n",
      "978      1  \n",
      "979      0  \n",
      "\n",
      "[980 rows x 76 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert the sparse matrix to a DataFrame\n",
    "df_bagofword = pd.DataFrame(X_bagofword.toarray(), columns=bagofword_vectorizer.get_feature_names_out())\n",
    "print(df_bagofword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     absolutely  advertised  amazing  arrived  away     awful  best  better  \\\n",
      "0      0.000000         0.0      0.0      0.0   0.0  0.000000   0.0     0.0   \n",
      "1      0.000000         0.0      0.0      0.0   0.0  0.000000   0.0     0.0   \n",
      "2      0.000000         0.0      0.0      0.0   0.0  0.000000   0.0     0.0   \n",
      "3      0.000000         0.0      0.0      0.0   0.0  0.551071   0.0     0.0   \n",
      "4      0.000000         0.0      0.0      0.0   0.0  0.000000   0.0     0.0   \n",
      "..          ...         ...      ...      ...   ...       ...   ...     ...   \n",
      "975    0.000000         0.0      0.0      0.0   0.0  0.000000   0.0     0.0   \n",
      "976    0.000000         0.0      0.0      0.0   0.0  0.000000   0.0     0.0   \n",
      "977    0.000000         0.0      0.0      0.0   0.0  0.000000   0.0     0.0   \n",
      "978    0.370199         0.0      0.0      0.0   0.0  0.000000   0.0     0.0   \n",
      "979    0.000000         0.0      0.0      0.0   0.0  0.000000   0.0     0.0   \n",
      "\n",
      "          bit  broke  ...  time  took  trust  week      wont      work  \\\n",
      "0    0.000000    0.0  ...   0.0   0.0    0.0   0.0  0.000000  0.000000   \n",
      "1    0.000000    0.0  ...   0.0   0.0    0.0   0.0  0.474759  0.000000   \n",
      "2    0.000000    0.5  ...   0.0   0.0    0.0   0.5  0.000000  0.000000   \n",
      "3    0.000000    0.0  ...   0.0   0.0    0.0   0.0  0.000000  0.000000   \n",
      "4    0.432693    0.0  ...   0.0   0.0    0.0   0.0  0.000000  0.000000   \n",
      "..        ...    ...  ...   ...   ...    ...   ...       ...       ...   \n",
      "975  0.000000    0.0  ...   0.0   0.0    0.0   0.0  0.000000  0.408995   \n",
      "976  0.000000    0.0  ...   0.0   0.0    0.0   0.0  0.000000  0.000000   \n",
      "977  0.000000    0.0  ...   0.0   0.0    0.0   0.0  0.000000  0.000000   \n",
      "978  0.000000    0.0  ...   0.0   0.0    0.0   0.0  0.000000  0.000000   \n",
      "979  0.000000    0.0  ...   0.0   0.0    0.0   0.0  0.000000  0.000000   \n",
      "\n",
      "        worst     worth  worthit     would  \n",
      "0    0.388662  0.000000      0.0  0.000000  \n",
      "1    0.000000  0.000000      0.0  0.000000  \n",
      "2    0.000000  0.000000      0.0  0.000000  \n",
      "3    0.000000  0.000000      0.0  0.000000  \n",
      "4    0.000000  0.432693      0.0  0.000000  \n",
      "..        ...       ...      ...       ...  \n",
      "975  0.000000  0.000000      0.0  0.000000  \n",
      "976  0.000000  0.000000      0.0  0.000000  \n",
      "977  0.000000  0.000000      0.0  0.000000  \n",
      "978  0.000000  0.000000      0.0  0.370199  \n",
      "979  0.000000  0.000000      0.0  0.000000  \n",
      "\n",
      "[980 rows x 76 columns]\n"
     ]
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=bagofword_vectorizer.get_feature_names_out())\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Aspect**           | **Bag of Words (BoW)**                                              | **TF-IDF**                                                             |\n",
    "|-----------------------|---------------------------------------------------------------------|-------------------------------------------------------------------------|\n",
    "| **Values**           | Raw word counts (e.g., 1, 2, etc.).                                 | Weighted scores based on term frequency and inverse document frequency. |\n",
    "| **Focus**            | Focuses on word frequency.                                          | Focuses on word importance in a document relative to the corpus.        |\n",
    "| **Common Words**     | Common words may dominate unless stopwords are removed.             | Common words are down-weighted automatically.                           |\n",
    "| **Interpretability** | Easier to interpret as it directly represents counts.               | Harder to interpret due to weighted values.                             |\n",
    "| **Use Case**         | Suitable for simple models or when frequency is sufficient.         | Suitable for tasks where word relevance matters more.                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Model Training & Evaluation (30 Marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><i>XGboost</b></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the dataset into training and testing sets (80/20) for both bag of word and TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size (Bag of Words): (784, 76)\n",
      "Testing set size (Bag of Words): (196, 76)\n",
      "Training set size (TF-IDF): (784, 76)\n",
      "Testing set size (TF-IDF): (196, 76)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_bagofword, X_test_bagofword, y_train_bagofword, y_test_bagofword = train_test_split(X_bagofword, df['sentiment'], test_size=0.2, random_state=42,shuffle=True, stratify=df['sentiment'])\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, df['sentiment'], test_size=0.2, random_state=42,shuffle=True,stratify=df['sentiment'])\n",
    "print(f\"Training set size (Bag of Words): {X_train_bagofword.shape}\")\n",
    "print(f\"Testing set size (Bag of Words): {X_test_bagofword.shape}\")\n",
    "print(f\"Training set size (TF-IDF): {X_train_tfidf.shape}\")\n",
    "print(f\"Testing set size (TF-IDF): {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for Bag of Words Model:\n",
      "Metric     Score     \n",
      "--------------------\n",
      "Accuracy   1.0000\n",
      "Precision  1.0000\n",
      "Recall     1.0000\n",
      "F1 Score   1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [23:50:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train a model using Bag of Words using XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize the model\n",
    "model_bagofword = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Train the model using the bag of words features\n",
    "model_bagofword.fit(X_train_bagofword, y_train_bagofword)\n",
    "\n",
    "# Make predictions on the test set of bag of words\n",
    "y_pred_bagofword = model_bagofword.predict(X_test_bagofword)\n",
    "\n",
    "#Evaluate models using accuracy, precision, recall, and F1-score.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Calculate metrics\n",
    "accuracy_bagofword = accuracy_score(y_test_bagofword, y_pred_bagofword)\n",
    "precision_bagofword = precision_score(y_test_bagofword, y_pred_bagofword, average='weighted')\n",
    "recall_bagofword = recall_score(y_test_bagofword, y_pred_bagofword, average='weighted')\n",
    "f1_bagofword = f1_score(y_test_bagofword, y_pred_bagofword, average='weighted')\n",
    "\n",
    "# Print metrics in a formatted table\n",
    "print(\"Evaluation Metrics for Bag of Words Model:\")\n",
    "print(f\"{'Metric':<10} {'Score':<10}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"{'Accuracy':<10} {accuracy_bagofword:.4f}\")\n",
    "print(f\"{'Precision':<10} {precision_bagofword:.4f}\")\n",
    "print(f\"{'Recall':<10} {recall_bagofword:.4f}\")\n",
    "print(f\"{'F1 Score':<10} {f1_bagofword:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for TF-IDF Model:\n",
      "Metric     Score     \n",
      "--------------------\n",
      "Accuracy   1.0000\n",
      "Precision  1.0000\n",
      "Recall     1.0000\n",
      "F1 Score   1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [23:50:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train a model using TF-IDF using XGBoost\n",
    "\n",
    "# Initialize the model\n",
    "model_tfidf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Train the model using the TF-IDF features\n",
    "model_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "# Make predictions on the test set of TF-IDF\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_tfidf = accuracy_score(y_test_tfidf, y_pred_tfidf)\n",
    "precision_tfidf = precision_score(y_test_tfidf, y_pred_tfidf, average='weighted')\n",
    "recall_tfidf = recall_score(y_test_tfidf, y_pred_tfidf, average='weighted')\n",
    "f1_tfidf = f1_score(y_test_tfidf, y_pred_tfidf, average='weighted')\n",
    "\n",
    "# Print metrics in a formatted table\n",
    "print(\"Evaluation Metrics for TF-IDF Model:\")\n",
    "print(f\"{'Metric':<10} {'Score':<10}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"{'Accuracy':<10} {accuracy_tfidf:.4f}\")\n",
    "print(f\"{'Precision':<10} {precision_tfidf:.4f}\")\n",
    "print(f\"{'Recall':<10} {recall_tfidf:.4f}\")\n",
    "print(f\"{'F1 Score':<10} {f1_tfidf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on unseen data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for unseen text using Bag of Words model:\n",
      "Text: This is a great product! I love it. | Predicted Sentiment: 1\n",
      "Text: I am not satisfied with the service. | Predicted Sentiment: 0\n",
      "Predictions for unseen text using TF-IDF model:\n",
      "Text: This is a great product! I love it. | Predicted Sentiment: 1\n",
      "Text: I am not satisfied with the service. | Predicted Sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "#Create a unseen text for prediction\n",
    "unseen_text = [\"This is a great product! I love it.\", \"I am not satisfied with the service.\"]\n",
    "\n",
    "# Clean the unseen text\n",
    "unseen_text_cleaned = [clean_text(text) for text in unseen_text]\n",
    "\n",
    "# Convert the cleaned text to lowercase\n",
    "unseen_text_cleaned = [text.lower() for text in unseen_text_cleaned]\n",
    "\n",
    "# Remove stopwords from the unseen text\n",
    "unseen_text_cleaned = [remove_stopwords(text) for text in unseen_text_cleaned]\n",
    "\n",
    "# Lemmatize the unseen text\n",
    "unseen_text_cleaned = [lemmatize_text(text) for text in unseen_text_cleaned]\n",
    "\n",
    "\n",
    "# Convert the cleaned text to Bag of Words features\n",
    "X_unseen_bagofword = bagofword_vectorizer.transform(unseen_text_cleaned)\n",
    "# Convert the cleaned text to TF-IDF features\n",
    "X_unseen_tfidf = tfidf_vectorizer.transform(unseen_text_cleaned)\n",
    "\n",
    "# Make predictions using the Bag of Words model\n",
    "y_pred_unseen_bagofword = model_bagofword.predict(X_unseen_bagofword)\n",
    "# Make predictions using the TF-IDF model\n",
    "y_pred_unseen_tfidf = model_tfidf.predict(X_unseen_tfidf)\n",
    "# Print the predictions\n",
    "print(\"Predictions for unseen text using Bag of Words model:\")\n",
    "for text, pred in zip(unseen_text, y_pred_unseen_bagofword):\n",
    "    print(f\"Text: {text} | Predicted Sentiment: {pred}\")\n",
    "print(\"Predictions for unseen text using TF-IDF model:\")\n",
    "for text, pred in zip(unseen_text, y_pred_unseen_tfidf):\n",
    "    print(f\"Text: {text} | Predicted Sentiment: {pred}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss results and select the best-performing model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
